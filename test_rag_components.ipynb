{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_BASE = {\n",
    "    \"doc1\": {\n",
    "        \"title\": \"Project Chimera Overview\",\n",
    "        \"content\": (\n",
    "            \"Project Chimera is a research initiative focused on developing \"\n",
    "            \"novel bio-integrated interfaces. It aims to merge biological \"\n",
    "            \"systems with advanced computing technologies.\"\n",
    "        )\n",
    "    },\n",
    "    \"doc2\": {\n",
    "        \"title\": \"Chimera's Neural Interface\",\n",
    "        \"content\": (\n",
    "            \"The core component of Project Chimera is a neural interface \"\n",
    "            \"that allows for bidirectional communication between the brain \"\n",
    "            \"and external devices. This interface uses biocompatible \"\n",
    "            \"nanomaterials.\"\n",
    "        )\n",
    "    },\n",
    "    \"doc3\": {\n",
    "        \"title\": \"Applications of Chimera\",\n",
    "        \"content\": (\n",
    "            \"Potential applications of Project Chimera include advanced \"\n",
    "            \"prosthetics, treatment of neurological disorders, and enhanced \"\n",
    "            \"human-computer interaction. Ethical considerations are paramount.\"\n",
    "        )\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 -> (Retrieval): Locating Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_retrieval(query, documents):\n",
    "    query_words = set(query.lower().split())\n",
    "    best_doc_id = None\n",
    "    best_overlap = 0\n",
    "    \n",
    "    for doc_id, doc in documents.items():\n",
    "        # Compare the query words with the document's content words\n",
    "        doc_words = set(doc[\"content\"].lower().split())\n",
    "        overlap = len(query_words.intersection(doc_words))\n",
    "        \n",
    "        if overlap > best_overlap:\n",
    "            best_overlap = overlap\n",
    "            best_doc_id = doc_id\n",
    "    \n",
    "    # Return the best document, or None if nothing matched\n",
    "    return documents.get(best_doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2 -> (Query Augmentation): Creating Context-Rich Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we enrich the users original question with the document with that document's content which will significantly reduces hallucinations because the language model sees both the question and real data.\n",
    "\"\"\"\n",
    "def rag_generation(query, document):\n",
    "    if document:\n",
    "        snippet = f\"{document['title']}: {document['content']}\"\n",
    "        prompt = f\"Using the following information: '{snippet}', answer: {query}\"\n",
    "    else:\n",
    "        prompt = f\"No relevant information found. Answer directly: {query}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4 -> (Generation): Producing Tailored Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "    \"\"\"\n",
    "    This function interfaces with a language model to generate a response based on the provided prompt.\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt (str): A string containing the question or task for the language model, potentially augmented with additional context.\n",
    "    \n",
    "    Returns:\n",
    "    - response (str): The generated text from the language model, which aims to answer the question or fulfill the task described in the prompt.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_generation(query):\n",
    "    # This approach ignores the knowledge base\n",
    "    prompt = f\"Answer directly the following query: {query}\"\n",
    "    return get_llm_response(prompt)\n",
    "#or \n",
    "def rag_generation(query, document):\n",
    "    # This approach augments the prompt via the knowledge base\n",
    "    if document:\n",
    "        snippet = f\"{document['title']}: {document['content']}\"\n",
    "        prompt = f\"Using the following information: '{snippet}', answer: {query}\"\n",
    "    else:\n",
    "        prompt = f\"No relevant information found. Answer directly: {query}\"\n",
    "    return get_llm_response(prompt)\n",
    "#the problem here in the rag generation is that we can have a document but it does not contain the relevant information needed so to make sure we use the llm properly we shoukd construct the model so that he can answer onl if the relevant information are present \n",
    "def rag_generation(query, document):\n",
    "    if document:\n",
    "        snipet = f\"{document['title']}:{document['content']}\"\n",
    "        prompt = f\"only answer {query} if all required info is present in the snippet: '{snipet}'\"\n",
    "    else:\n",
    "        prompt = f\"Refuse politely if No relevant information found. Answer directly: {query}\"\n",
    "    return get_llm_response(prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main goal of Project Chimera?\"\n",
    "\n",
    "naive_answer = naive_generation(query)\n",
    "print(\"Naive approach:\", naive_answer)\n",
    "\n",
    "doc = rag_retrieval(query, KNOWLEDGE_BASE)\n",
    "rag_answer = rag_generation(query, doc)\n",
    "print(\"RAG approach:\", rag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "    naive_generation(query) can easily lead to random or inaccurate answers regarding “Project Chimera”, such as: \"The main goal of Project Chimera is to develop advanced artificial intelligence systems that can enhance human capabilities and improve decision-making processes across various fields.\".\n",
    "    rag_generation(query, doc) **provides contextual information** from the knowledge base, ensuring the answer is grounded: \"The main goal of Project Chimera is to enable bidirectional communication between the brain and external devices through the use of a neural interface.\".\n",
    "    Seeing both approaches (and their actual output) helps you compare how naive answers can deviate from your authoritative data, while RAG-based responses stay closer to truth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Fighting Hallucinations: RAG's Role in Trustworthy LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
